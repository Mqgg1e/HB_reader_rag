# 英文圣经 RAG 项目：数据准备文档

## 概述

本文件总结了英文圣经 RAG (Retrieval Augmented Generation) 项目中**数据准备阶段**所执行的步骤。目标是将原始的 TXT 格式圣经文本转化为结构化、可用于生成文本嵌入并构建向量数据库的格式。

------

## 1. 文本读取 (Text Reading)

**目标**: 从原始 TXT 文件中高效、健壮地读取圣经内容。

**方法**:

- 使用 Python 内置的文件 I/O 操作 (`open()`)。
- 强制使用 `UTF-8` 编码以确保正确处理所有字符。
- 实现了一个**健壮的文件路径处理函数 (read_text_file_robustly)**，它能根据当前脚本文件所在的目录或程序执行时的当前工作目录灵活地查找文件。这确保了无论在何种环境下运行代码，都能准确找到并读取 `AKJV.txt` 文件。
- 包含了错误处理机制，以捕获 `FileNotFoundError` 和 `IOError`。

**关键代码点**:

- `os.path.dirname(os.path.abspath(__file__))`: 获取当前脚本所在目录。
- `os.path.join()`: 安全地拼接文件路径，兼容不同操作系统。
- `with open(file_path, 'r', encoding='utf-8') as f:`: 标准的文件读取模式。

------

## 2. 文本清洗 (Text Cleaning)

**目标**: 从原始文本中移除所有非圣经正文的内容，特别是文件头。

**方法**:

- 识别并利用文件中的特定分隔符 `"--------------------------------------------------------------------------------"`。
- 假定圣经正文内容位于第二个分隔符之后。
- 实现了 **clean_bible_text_header 函数** 来执行此操作。
- 移除了清洗后文本开头可能存在的额外空白行。

**关键代码点**:

- `raw_text.split(delimiter)`: 根据分隔符将文本分割成多个部分。
- `parts[2].strip()`: 提取并清理包含圣经正文的部分。
- `re.sub(r'^\s*\n', '', text, flags=re.MULTILINE)`: 使用正则表达式移除前导空行。

------

## 3. 文本结构化与元数据提取 (Text Structuring and Metadata Extraction)

**目标**: 将清洗后的圣经文本解析成单独的“节”(verse)，并为每节经文提取精确的位置元数据（书名、章节、节号）。

**方法**:

- 定义了一个简化的 `Document` 类来封装每节经文的 `page_content` (内容) 和 `metadata` (元数据)。
- **逐行处理**清洗后的文本。
- **识别书名行**: 简单启发式判断（例如，长度短且不符合经文模式的行）来识别当前正在处理的书的完整名称（如 "Genesis"）。
- **核心解析**: 使用**正则表达式 (\d?[A-Za-z]+)\.(\d+):(\d+)\s(.\*)** 来精确匹配每一节经文的格式：
  - `(\d?[A-Za-z]+)`: 捕获带数字或不带数字的书名缩写（如 "Gen", "1Sam"）。
  - `(\d+)`: 捕获章节号和节号。
  - `(.*)`: 捕获实际的经文内容。
- **创建书名缩写映射**: 维护一个 `book_abbr_to_full_name` 字典，将圣经缩写（如 "Gen"）映射到完整的书名（"Genesis"），确保元数据的准确性。
- 将提取到的信息（书名、章节、节号）组织成一个 `metadata` 字典，并与经文内容一起存储在 `Document` 对象中。为后续检索提供了**人类可读的 location 字段（例如 "Genesis 1:1"）**。

**关键代码点**:

- `parse_bible_verses(cleaned_text)` 函数。
- `re.compile(r'(\d?[A-Za-z]+)\.(\d+):(\d+)\s(.*)')`: 核心正则表达式。
- `book_abbr_to_full_name` 字典。
- `Document` 类的使用及其 `metadata` 字典。

------

## 4. 数据持久化 (Data Persistence)

**目标**: 将清洗并结构化后的 `Document` 对象列表保存到文件中，以便于后续步骤直接加载，避免重复处理，并提高效率。

**方法**:

- 选择了 **JSON Lines (JSONL)** 格式作为存储方式，因为它既结构化又易于读写。
- 实现了 **save_documents_to_jsonl 函数**：
  - 将每个 `Document` 对象通过 `to_dict()` 方法转换为字典，然后序列化为 JSON 字符串。
  - 确保输出目录存在，如果不存在则自动创建。
- 提供了 **load_documents_from_jsonl 函数**：
  - 用于从 JSONL 文件中反向加载数据，重建 `Document` 对象列表。

**关键代码点**:

- `Document.to_dict()` 方法。
- `json.dumps()`: 将 Python 字典序列化为 JSON 字符串。
- `json.loads()`: 将 JSON 字符串反序列化为 Python 字典。
- `os.makedirs()`: 确保文件保存目录存在。





## 第二阶段：文本嵌入与向量数据库构建

### 目标

将圣经经文转换为数值向量（嵌入），并使用这些向量构建一个高效的向量数据库，以便进行语义相似性搜索。

### 成果

- **选择嵌入模型**: 经过评估和选择，最终决定使用 **BAAI/bge-large-en-v1.5** 模型来生成高质量的英文文本嵌入。该模型以其卓越的性能和适中的资源占用（约 1.3GB 模型大小，建议 1.5GB-2GB 内存，推荐 GPU）而备受青睐。
- **向量数据库**: 使用 **FAISS (Facebook AI Similarity Search)** 作为向量数据库。FAISS 提供了高效的相似性搜索和管理嵌入的能力。
- **构建与保存**: 成功地为所有 31102 节圣经经文生成了嵌入向量，并将这些向量连同其原始 `Document` 对象一起存储到 FAISS 索引中。该索引被保存为本地文件（`.faiss` 和 `.pkl` 文件），确保了数据的持久性和可重复加载。
- **资源监控**: 在向量数据库构建过程中，我们加入了时间与内存资源监控，以量化每个关键步骤的性能开销，为未来的优化提供数据基础。

### 关键组件

- **LangChain**: 作为主要框架，用于简化 RAG 流程的构建。
- **langchain_huggingface.HuggingFaceEmbeddings**: 用于加载和使用 `BAAI/bge-large-en-v1.5` 模型。
- **langchain_community.vectorstores.FAISS**: 用于构建和管理向量数据库。
- **psutil**: 用于性能监控。

## 当前状态

目前，我们已经拥有了一个准备就绪的圣经语义知识库，它可以根据语义相似性快速检索相关经文。下一步将是把这个检索能力与大型语言模型结合起来，形成完整的问答系统。